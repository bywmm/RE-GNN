{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangjunfu/env/anaconda3/envs/geometric/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import networkx as nx\n",
    "import utils.preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from utils.data import load_glove_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_prefix = 'data/preprocessed/ACM_processed/'\n",
    "num_ntypes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "paper_label = pd.read_csv('data/raw/ACM/paper_label.txt', sep='\\t', header=None, names=['paper_id', 'label', 'paper_abstract'], keep_default_na=False, encoding='utf-8')\n",
    "\n",
    "paper_author = pd.read_csv('data/raw/ACM/paper_author.txt', sep='\\t', header=None, names=['paper_id', 'author_id'], keep_default_na=False, encoding='utf-8')\n",
    "paper_paper = pd.read_csv('data/raw/ACM/paper_paper.txt', sep='\\t', header=None, names=['paper_id_1', 'paper_id_2'], keep_default_na=False, encoding='utf-8')\n",
    "paper_subject = pd.read_csv('data/raw/ACM/paper_subject.txt', sep='\\t', header=None, names=['paper_id', 'subject_id'], keep_default_na=False, encoding='utf-8')\n",
    "papers = pd.read_csv('data/raw/ACM/paper.txt', sep='\\t', header=None, names=['paper_id', 'paper_abstract'], keep_default_na=False, encoding='cp1252')\n",
    "authors = pd.read_csv('data/raw/ACM/author.txt', sep='\\t', header=None, names=['author_id', 'author_name'], keep_default_na=False, encoding='utf-8')\n",
    "subjects = pd.read_csv('data/raw/ACM/subject.txt', sep='\\t', header=None, names=['subject_id', 'subject'], keep_default_na=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4019, 3)\n",
      "(4019, 2)\n",
      "(60, 2)\n",
      "(7167, 2)\n",
      "(9615, 2)\n",
      "(4019, 2)\n",
      "(13407, 2)\n"
     ]
    }
   ],
   "source": [
    "# filter out all nodes which does not associated with labeled papers\n",
    "labeled_papers = paper_label['paper_id'].to_list()\n",
    "paper_author = paper_author[paper_author['paper_id'].isin(labeled_papers)].reset_index(drop=True)\n",
    "valid_papers = paper_author['paper_id'].unique()\n",
    "valid_authors = paper_author['author_id'].unique()\n",
    "authors = authors[authors['author_id'].isin(valid_authors)].reset_index(drop=True)\n",
    "\n",
    "papers = papers[papers['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_label = paper_label[paper_label['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_paper = paper_paper[paper_paper['paper_id_1'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_paper = paper_paper[paper_paper['paper_id_2'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_subject = paper_subject[paper_subject['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "valid_subjects = paper_subject['subject_id'].unique()\n",
    "subjects = subjects[subjects['subject_id'].isin(valid_subjects)].reset_index(drop=True)\n",
    "print(paper_label.shape)\n",
    "print(papers.shape)\n",
    "print(subjects.shape)\n",
    "print(authors.shape)\n",
    "print(paper_paper.shape)\n",
    "print(paper_subject.shape)\n",
    "print(paper_author.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "paper_label = paper_label.sort_values('paper_id').reset_index(drop=True)\n",
    "authors = authors.sort_values('author_id').reset_index(drop=True)\n",
    "subjects = subjects.sort_values('subject_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extract labels of authors\n",
    "labels = paper_label['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11246, 11246)\n",
      "53991\n"
     ]
    }
   ],
   "source": [
    "# build the adjacency matrix for the graph consisting of authors, papers, terms and conferences\n",
    "# 0 for authors, 1 for papers, 2 for terms, 3 for conferences\n",
    "dim = len(paper_label) + len(authors) + len(subjects)\n",
    "type_mask = np.zeros((dim), dtype=int)\n",
    "type_mask[len(paper_label):len(paper_label)+len(authors)] = 1\n",
    "type_mask[len(paper_label)+len(authors):] = 2\n",
    "\n",
    "paper_id_mapping = {row['paper_id']: i for i, row in paper_label.iterrows()}\n",
    "author_id_mapping = {row['author_id']: i + len(paper_label) for i, row in authors.iterrows()}\n",
    "subject_id_mapping = {row['subject_id']: i + len(paper_label) + len(authors) for i, row in subjects.iterrows()}\n",
    "\n",
    "adjM = np.zeros((dim, dim), dtype=int)\n",
    "for _, row in paper_paper.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id_1']]\n",
    "    idx2 = paper_id_mapping[row['paper_id_2']]\n",
    "    adjM[idx1, idx2] = 1\n",
    "    adjM[idx2, idx1] = 1\n",
    "#     adjM[idx2, idx1] = 1\n",
    "for _, row in paper_author.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = author_id_mapping[row['author_id']]\n",
    "    adjM[idx1, idx2] = 1\n",
    "    adjM[idx2, idx1] = 1\n",
    "for _, row in paper_subject.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = subject_id_mapping[row['subject_id']]\n",
    "    adjM[idx1, idx2] = 1\n",
    "    adjM[idx2, idx1] = 1\n",
    "    \n",
    "print(scipy.sparse.csr_matrix(adjM).shape)\n",
    "print(scipy.sparse.csr_matrix(adjM).getnnz())\n",
    "    \n",
    "scipy.sparse.save_npz(save_prefix + 'adjM.npz', scipy.sparse.csr_matrix(adjM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11246, 11246)\n",
      "53991\n"
     ]
    }
   ],
   "source": [
    "adjMM = np.zeros((dim, dim), dtype=int)\n",
    "for _, row in paper_paper.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id_1']]\n",
    "    idx2 = paper_id_mapping[row['paper_id_2']]\n",
    "    adjMM[idx1, idx2] = 1\n",
    "    adjMM[idx2, idx1] = 1\n",
    "for _, row in paper_author.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = author_id_mapping[row['author_id']]\n",
    "    adjMM[idx1, idx2] = 2\n",
    "    adjMM[idx2, idx1] = 3\n",
    "for _, row in paper_subject.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = subject_id_mapping[row['subject_id']]\n",
    "    adjMM[idx1, idx2] = 4\n",
    "    adjMM[idx2, idx1] = 5\n",
    "    \n",
    "print(scipy.sparse.csr_matrix(adjMM).shape)\n",
    "print(scipy.sparse.csr_matrix(adjMM).getnnz())\n",
    "scipy.sparse.save_npz(save_prefix + 'adjMM.npz', scipy.sparse.csr_matrix(adjMM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65226\n",
      "65226\n"
     ]
    }
   ],
   "source": [
    "num_etype = adjMM.max()\n",
    "for i in range(adjMM.shape[0]):\n",
    "#     if (i, i) not in adjMM:\n",
    "    adjMM[(i, i)] = num_etype + 1\n",
    "print(scipy.sparse.csr_matrix(adjMM).getnnz())\n",
    "scipy.sparse.save_npz(save_prefix + 'adjMM_wsl.npz', scipy.sparse.csr_matrix(adjMM))\n",
    "\n",
    " \n",
    "for i in range(adjMM.shape[0]):\n",
    "#     if (i, i) not in adjMM:\n",
    "    adjMM[(i, i)] = num_etype + 1 + type_mask[i]\n",
    "print(scipy.sparse.csr_matrix(adjMM).getnnz())\n",
    "scipy.sparse.save_npz(save_prefix + 'adjMM_wsl_2.npz', scipy.sparse.csr_matrix(adjMM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use HAN paper's preprocessed data as the features of authors (https://github.com/Jhy1993/HAN)\n",
    "mat = scipy.io.loadmat('data/raw/DBLP/DBLP4057_GAT_with_idx.mat')\n",
    "features_author = np.array(list(zip(*sorted(zip(labeled_authors, mat['features']), key=lambda tup: tup[0])))[1])\n",
    "features_author = scipy.sparse.csr_matrix(features_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xyfu\\Anaconda3\\envs\\dgl-gpu\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'doe', 'ha', 'le', \"n't\", 'need', 'sha', 'u', 'wa', 'wo'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# use bag-of-words representation of paper titles as the features of papers\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "vectorizer = CountVectorizer(min_df=2, stop_words=stopwords, tokenizer=LemmaTokenizer())\n",
    "features_paper = vectorizer.fit_transform(papers['paper_title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use pretrained GloVe vectors as the features of terms\n",
    "features_term = np.zeros((len(terms), glove_dim))\n",
    "for i, row in terms.iterrows():\n",
    "    features_term[i] = glove_vectors.get(row['term'], glove_vectors['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32789, 3)\n",
      "(41633537, 5)\n",
      "(30803571, 5)\n"
     ]
    }
   ],
   "source": [
    "expected_metapaths = [\n",
    "    [(0, 1, 0), (0, 1, 2, 1, 0), (0, 1, 3, 1, 0)],\n",
    "    [(1, 0, 1), (1, 2, 1), (1, 3, 1)],\n",
    "    [(2, 1, 2), (2, 1, 0, 1, 2), (2, 1, 3, 1, 2)],\n",
    "    [(3, 1, 3), (3, 1, 0, 1, 3), (3, 1, 2, 1, 3)]\n",
    "]\n",
    "# create the directories if they do not exist\n",
    "for i in range(1):\n",
    "    pathlib.Path(save_prefix + '{}'.format(i)).mkdir(parents=True, exist_ok=True)\n",
    "for i in range(1):\n",
    "    # get metapath based neighbor pairs\n",
    "    neighbor_pairs = utils.preprocess.get_metapath_neighbor_pairs(adjM, type_mask, expected_metapaths[i])\n",
    "    # construct and save metapath-based networks\n",
    "    G_list = utils.preprocess.get_networkx_graph(neighbor_pairs, type_mask, i)\n",
    "    \n",
    "    # save data\n",
    "    # networkx graph (metapath specific)\n",
    "    for G, metapath in zip(G_list, expected_metapaths[i]):\n",
    "        nx.write_adjlist(G, save_prefix + '{}/'.format(i) + '-'.join(map(str, metapath)) + '.adjlist')\n",
    "    # node indices of edge metapaths\n",
    "    all_edge_metapath_idx_array = utils.preprocess.get_edge_metapath_idx_array(neighbor_pairs)\n",
    "    for metapath, edge_metapath_idx_array in zip(expected_metapaths[i], all_edge_metapath_idx_array):\n",
    "        np.save(save_prefix + '{}/'.format(i) + '-'.join(map(str, metapath)) + '_idx.npy', edge_metapath_idx_array)\n",
    "# save data\n",
    "# all nodes adjacency matrix\n",
    "scipy.sparse.save_npz(save_prefix + 'adjM.npz', scipy.sparse.csr_matrix(adjM))\n",
    "# all nodes (authors, papers, terms and conferences) features\n",
    "# currently only have features of authors, papers and terms\n",
    "scipy.sparse.save_npz(save_prefix + 'features_{}.npz'.format(0), features_author)\n",
    "scipy.sparse.save_npz(save_prefix + 'features_{}.npz'.format(1), features_paper)\n",
    "np.save(save_prefix + 'features_{}.npy'.format(2), features_term)\n",
    "# all nodes (authors, papers, terms and conferences) type labels\n",
    "np.save(save_prefix + 'node_types.npy', type_mask)\n",
    "# author labels\n",
    "np.save(save_prefix + 'labels.npy', labels)\n",
    "# author train/validation/test splits\n",
    "rand_seed = 1566911444\n",
    "train_idx, val_idx = train_test_split(np.arange(len(labels)), test_size=400, random_state=rand_seed)\n",
    "train_idx, test_idx = train_test_split(train_idx, test_size=3257, random_state=rand_seed)\n",
    "train_idx.sort()\n",
    "val_idx.sort()\n",
    "test_idx.sort()\n",
    "np.savez(save_prefix + 'train_val_test_idx.npz',\n",
    "         val_idx=val_idx,\n",
    "         train_idx=train_idx,\n",
    "         test_idx=test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# post-processing for mini-batched training\n",
    "target_idx_list = np.arange(4057)\n",
    "for metapath in [(0, 1, 0), (0, 1, 2, 1, 0), (0, 1, 3, 1, 0)]:\n",
    "    edge_metapath_idx_array = np.load(save_prefix + '{}/'.format(0) + '-'.join(map(str, metapath)) + '_idx.npy')\n",
    "    target_metapaths_mapping = {}\n",
    "    for target_idx in target_idx_list:\n",
    "        target_metapaths_mapping[target_idx] = edge_metapath_idx_array[edge_metapath_idx_array[:, 0] == target_idx][:, ::-1]\n",
    "    out_file = open(save_prefix + '{}/'.format(0) + '-'.join(map(str, metapath)) + '_idx.pickle', 'wb')\n",
    "    pickle.dump(target_metapaths_mapping, out_file)\n",
    "    out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('geometric')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "55b6f13e4ed1e84424375e0dd3b17799d86e35602356f05193dc78222d9cf425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
